# -*- coding: utf-8 -*-
"""BCI_ML_Cross_Subject_EEG_ITNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EZEW2j0VTZcJEaPwIS8adK1lWsF17FZF
"""

pip install mne

pip install tensorflow_addons

"""# Data Loading (SEED-IV): Load the extracted and transformed data

"""

import scipy.io
import pandas as pd
import numpy as np
import os
import glob
import matplotlib.pyplot as plt
import warnings
import scipy.signal as signal
import tensorflow as tf
from sklearn.preprocessing import StandardScaler

# Ignore warnings
warnings.filterwarnings('ignore')

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('CE807/BCITL/Data/eeg_raw_data')
GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
print('List session files: ', os.listdir(GOOGLE_DRIVE_PATH))

import sys
sys.path.append('/content/gdrive/My Drive/CE807/BCITL/Code')
import tensorflow as tf
import tl_helper

from sklearn import metrics
from keras.callbacks import EarlyStopping, ModelCheckpoint
from tl_helper import preprocess_eeg, split_eeg
from tl_helper import process_file, divide_data
from tl_helper import get_session_labels ,get_categorical_labels
from tl_helper import get_participant_files
from tl_helper import get_session_number, create_raw_eeg_object
from tl_helper import  process_eeg_chunks_for_EEGNET
from tl_helper import EEGNet,  EEGNet_FirstLayerOnly, create_frozen_EEG_ITNet
print ("Import tl_helper sucessfully")

from sklearn import metrics
from keras.callbacks import EarlyStopping, ModelCheckpoint

import tensorflow as tf
def get_categorical_labels_Cross(label_list_S1, label_list_S2, label_list_S3):
    categorical_labels_S1 = tf.keras.utils.to_categorical(label_list_S1)
    #print("categorical_labels_S1:", categorical_labels_S1.shape)

    categorical_labels_S2 = tf.keras.utils.to_categorical(label_list_S2)
    #print("categorical_labels_S1:", categorical_labels_S1.shape)

    categorical_labels_S3 = tf.keras.utils.to_categorical(label_list_S3)
    #print("categorical_labels_S1:", categorical_labels_S1.shape)

    return categorical_labels_S1, categorical_labels_S2, categorical_labels_S3

def data_preprocessing_splitting(participant_files, num_participants, participant_num, n_chunks):
    session1_label, session2_label, session3_label = get_session_labels()
    print("  *** **** ")
    print("Participant Data:", participant_num)

    for session_num in range(1, 4):  # Loop through session 1, session 2, and session 3
        # Find the appropriate session label based on the current session number

        # Get the files for the current participant
        files = participant_files[participant_num]
        # Sort the files based on the session number and get session number from path
        sorted_files = sorted(files, key=get_session_number)

        file_path = sorted_files[session_num - 1]  # Adjust the index since sessions are 1-indexed

        if session_num == 1:
            print("Session 1 file_path session1_label: ", file_path)
            eeg_chunks_list_train_S1, label_list_S1 = process_file(file_path, session1_label, n_chunks)

        elif session_num == 2:
            print("Session 2 file_path session2_label: ", file_path)
            eeg_chunks_list_train_S2, label_list_S2 = process_file(file_path, session2_label, n_chunks)

        elif session_num == 3:
            print("Session 3 file_path session3_label: ", file_path)
            eeg_chunks_list_train_S3, label_list_S3 = process_file(file_path, session3_label, n_chunks)

            # For Participant i - Session 1

            print(" *** ** Participant Data Summary ** *** : ",  participant_num,  "    *******   ")

            print("Participant - Session 1")
            print("No of Trails (Train) ", len(eeg_chunks_list_train_S1))
            print("Shape of each eeg_chunks ", eeg_chunks_list_train_S1[0].shape)
            print("----------------------")

            # For Participant i - Session 2
            print("Participant - Session 2")
            print("No of Trails (Train) ", len(eeg_chunks_list_train_S2))
            print("Shape of each eeg_chunks ", eeg_chunks_list_train_S2[0].shape)
            print("----------------------")

            # For Participant i - Session 3
            print("Participant - Session 3")
            print("No of Trails (Train) ", len(eeg_chunks_list_train_S3))
            print("Shape of each eeg_chunks ", eeg_chunks_list_train_S3[0].shape)
            print("----------------------")

            print("-----------process_eeg_chunks_for_Modeling-----------")

            ChunkData_session1 = process_eeg_chunks_for_EEGNET(eeg_chunks_list_train_S1)
            ChunkData_session2 = process_eeg_chunks_for_EEGNET(eeg_chunks_list_train_S2)
            ChunkData_session3 = process_eeg_chunks_for_EEGNET(eeg_chunks_list_train_S3)

            categorical_labels, categorical_labels_2, categorical_labels_3 = get_categorical_labels_Cross(label_list_S1, label_list_S2, label_list_S3)
            print("ChunkData_session1_train : ", ChunkData_session1.shape)
            print("ChunkData_session2_train : ", ChunkData_session2.shape)
            print("ChunkData_session3_train : ", ChunkData_session3.shape)



            print("-----------data_preprocessing_splitting_done-----------")
    # Return the required variables
    return (
        ChunkData_session1,
        ChunkData_session2,
        ChunkData_session3,
        categorical_labels,
        categorical_labels_2,
        categorical_labels_3

    )

"""@Pipeline"""

def chunk_data(GOOGLE_DRIVE_PATH, num_participants, n_chunks):
    # Assuming get_participant_files and data_preprocessing_splitting functions are defined

    participant_files = get_participant_files(GOOGLE_DRIVE_PATH)
    print("participant_files: ", participant_files)

    # Define the list to store the returned values for each participant
    results_per_participant = []

    # Loop through participants from 1 to num_participants
    for participant_num in range(1, num_participants + 1):
        print(f"Participant {participant_num}")
        # Call the function and store the returned values
        (
            ChunkData_session1,
            ChunkData_session2,
            ChunkData_session3,
            categorical_labels,
            categorical_labels_2,
            categorical_labels_3
        ) = data_preprocessing_splitting(participant_files, num_participants, participant_num, n_chunks)

        # Append the results to the list
        results_per_participant.append((
            ChunkData_session1,
            ChunkData_session2,
            ChunkData_session3,
            categorical_labels,
            categorical_labels_2,
            categorical_labels_3
        ))
       # Print the shapes of ChunkData for each participant
    for participant_num, results in enumerate(results_per_participant, start=1):

      ChunkData_session1, ChunkData_session2, ChunkData_session3, _, _, _ = results
      print(f"Participant {participant_num} - ChunkData_session1_train: {ChunkData_session1.shape} categorical_labels: {categorical_labels.shape}")
      print(f"Participant {participant_num} - ChunkData_session2_train: {ChunkData_session2.shape}categorical_labels_2: {categorical_labels_2.shape}")
      print(f"Participant {participant_num} - ChunkData_session3_train: {ChunkData_session3.shape}categorical_labels_3: {categorical_labels_3.shape}")

    return results_per_participant

import tensorflow as tf
def get_categorical_labels_cross(label_list_S1, label_list_train_S2):
    categorical_labels_S1 = tf.keras.utils.to_categorical(label_list_S1)
    #print("categorical_labels_S1:", categorical_labels_S1.shape)

    categorical_labels_S2_train = tf.keras.utils.to_categorical(label_list_train_S2)
    #print("categorical_labels_S2_train:", categorical_labels_S2_train.shape)

    return categorical_labels_S1, categorical_labels_S2_train

import numpy as np

def concatenate_data(participant_num, results_per_participant):
    test_data = []
    test_labels = []
    train_data = []
    train_labels = []

    for idx, results in enumerate(results_per_participant):
        if idx + 1 == participant_num:
            ChunkData_session1, categorical_labels = results
            test_data.extend([ChunkData_session1])
            test_labels.extend([categorical_labels])
        else:
            train_data.extend([results[0]])
            train_labels.extend([results[1]])
        del results

    test_data = np.concatenate(test_data, axis=0)
    test_labels = np.concatenate(test_labels, axis=0)
    train_data = np.concatenate(train_data, axis=0)
    train_labels = np.concatenate(train_labels, axis=0)

    test_categorical_labels, train_categorical_labels= get_categorical_labels_cross(test_labels, train_labels)


    return test_data, test_labels, train_data, train_labels

import numpy as np
from sklearn import metrics
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model

def modeling_EEG_ITNet(test_data, test_labels, train_data, train_labels,participant_num, Chans, Samples):

    # Print data shapes
    print("   **********   ")
    print("   ***** Data Arrangement*****   ")


    print(f"test data shape for Participant  {test_data.shape}")
    print(f"test labels shape for Participant {test_labels.shape}")
    print(f"train data shape for Participants {train_data.shape}")
    print(f"train labels shape for Participants {train_labels.shape}")


    # Build the model
    model = EEG_ITNet(Chans, Samples)
    nb_classes=4
    #model = EEGNet(nb_classes, Chans, Samples)
    print("model.summary()")
    model.summary()

    # Compile the model
    learning_rate = 0.0001
    optimizer = Adam(learning_rate=learning_rate)
    #model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


    # Define callbacks
    early_stopping = EarlyStopping(monitor='val_loss', patience=100, mode='min', verbose=1, restore_best_weights=True)
    model_filename = f"trained_model_participant_{participant_num}.h5"
    model_checkpoint = ModelCheckpoint(model_filename, monitor='val_loss', save_best_only=True, mode='min')
    #callbacks = [model_checkpoint]
    callbacks = [early_stopping, model_checkpoint]


    # Train the model
    fitted = model.fit(x=train_data, y=train_labels, epochs=400, validation_split=0.2, verbose=1, callbacks=callbacks, batch_size=128)
    plot_training_history(fitted)
    tf.keras.backend.clear_session()

    #model = load_model(model_filename)

    # Evaluate the model
    predicted = model.predict(x=test_data)
    tune_labels_multiclass = np.argmax(test_labels, axis=1)
    predicted_labels_multiclass = np.argmax(predicted, axis=1)

    # Calculate metrics
    accuracy = metrics.accuracy_score(tune_labels_multiclass, predicted_labels_multiclass)
    print("Accuracy:", accuracy)

    f1_score = metrics.f1_score(tune_labels_multiclass, predicted_labels_multiclass, average='weighted')
    print("F1-score:", f1_score)

    classification_report = metrics.classification_report(tune_labels_multiclass, predicted_labels_multiclass)
    print("Classification Report:\n", classification_report)

"""###(Colab  Enviroment )"""

import pickle

def load_chunks_from_file(pickle_file_path):
    # Load the participant data from the pickle file
    with open(pickle_file_path, 'rb') as file:
        participant_data = pickle.load(file)

    return participant_data

import os
import pickle

def load_participant_data(pickle_files_directory):
    # List all pickle files in the directory
    pickle_files = [file for file in os.listdir(pickle_files_directory) if file.endswith('.pkl')]

    # Define a list to store loaded participant data
    loaded_results_per_participant = []

    # Loop through each participant's pickle file
    for participant_file in pickle_files:
        pickle_file_path = os.path.join(pickle_files_directory, participant_file)

        # Load the data for the participant
        loaded_participant_data = load_chunks_from_file(pickle_file_path)

        # Append the loaded data to the list
        loaded_results_per_participant.append((
            loaded_participant_data["ChunkData_session1_train"],
            #loaded_participant_data["ChunkData_session2_train"],
            #loaded_participant_data["ChunkData_session3_train"],
            loaded_participant_data["categorical_labels"],
            #loaded_participant_data["categorical_labels_2"],
            #loaded_participant_data["categorical_labels_3"]
        ))

    return loaded_results_per_participant

"""Load"""

# Specify the path to the directory containing participant pickle files
pickle_files_directory = os.path.join('gdrive', 'MyDrive', 'CE807/BCITL/Data/eeg_raw_data')

# Load participant data using the function
loaded_results_per_participant = load_participant_data(pickle_files_directory)

# Print the shapes of the loaded data for each participant
for participant_num, results in enumerate(loaded_results_per_participant, start=1):
    #ChunkData_session1, ChunkData_session2, ChunkData_session3, categorical_labels, categorical_labels_2, categorical_labels_3 = results
    ChunkData_session1, categorical_labels = results

    print(f"Participant {participant_num} - ChunkData_session1_train: {ChunkData_session1.shape} categorical_labels: {categorical_labels.shape}")
    #print(f"Participant {participant_num} - ChunkData_session2_train: {ChunkData_session2.shape} categorical_labels_2: {categorical_labels_2.shape}")
    #print(f"Participant {participant_num} - ChunkData_session3_train: {ChunkData_session3.shape} categorical_labels_3: {categorical_labels_3.shape}")
    print("\n")

def divide_participant(loaded_results_per_participant, participant_num):
    test_participant_data = loaded_results_per_participant[participant_num]

    train_participant_data = []
    for idx, participant_data in enumerate(loaded_results_per_participant):
        if idx != participant_num:
            train_participant_data.append((
                participant_data[0],  # ChunkData_session1_train
                #participant_data[1],  # ChunkData_session2_train
                #participant_data[2],  # ChunkData_session3_train
                participant_data[3],  # categorical_labels
                #participant_data[4],  # categorical_labels_2
                #participant_data[5]   # categorical_labels_3
            ))

    return test_participant_data, train_participant_data

participant_total = 15
n_chunks = 700
participant_num = 5
Chans = 62

test_data, test_labels, train_data, train_labels = concatenate_data(participant_num, loaded_results_per_participant)

print(f"test data shape for Participant {participant_num}: {test_data.shape}")
print(f"test labels shape for Participant {participant_num}: {test_labels.shape}")
print(f"train data shape for Participants: {train_data.shape}")
print(f"train labels shape for Participants {train_labels.shape}")

participant_total = 15
n_chunks = 700
participant_num = 9
Chans = 62

test_data, test_labels, train_data, train_labels = concatenate_data(participant_num, loaded_results_per_participant)
print(f"test data shape for Participant {participant_num}: {test_data.shape}")
print(f"test labels shape for Participant {participant_num}: {test_labels.shape}")
print(f"train data shape for Participants: {train_data.shape}")
print(f"train labels shape for Participants {train_labels.shape}")

modeling_EEG_ITNet(test_data, test_labels, train_data, train_labels, participant_num, Chans, n_chunks)

"""### EEGNET // https://arxiv.org/abs/1611.08024
### EEGITNET  //https://arxiv.org/abs/2204.06947

EXTRA
"""

import os
import pickle
from google.colab import drive

def save_chunks_in_file(participant_total, n_chunks, participant_num):
    Chans = 62
    Samples = 700

    print(f"Chans: {Chans}")
    print(f"Samples: {Samples}")
    print(f"participant_total: {participant_total}")
    print(f"n_chunks: {n_chunks}")

    # Mount Google Drive
    drive.mount('/content/gdrive', force_remount=True)
    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('CE807/BCITL/Data/eeg_raw_data')
    GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)
    print('List session files: ', os.listdir(GOOGLE_DRIVE_PATH))

    # Check if the path exists
    if os.path.exists(GOOGLE_DRIVE_PATH):
        print('List session files:', os.listdir(GOOGLE_DRIVE_PATH))
    else:
        print('Path does not exist:', GOOGLE_DRIVE_PATH)

    # Assuming chunk_data is defined somewhere
    results_per_participant = chunk_data(GOOGLE_DRIVE_PATH, participant_total, n_chunks)

    # Save data for each participant
    for participant_num, results in enumerate(results_per_participant, start=1):
        ChunkData_session1, ChunkData_session2, ChunkData_session3, categorical_labels, categorical_labels_2, categorical_labels_3 = results

        participant_data = {
            "ChunkData_session1_train": ChunkData_session1,
            "ChunkData_session2_train": ChunkData_session2,
            "ChunkData_session3_train": ChunkData_session3,
            "categorical_labels": categorical_labels,
            "categorical_labels_2": categorical_labels_2,
            "categorical_labels_3": categorical_labels_3,
        }


        participant_file_path = os.path.join(GOOGLE_DRIVE_PATH, f'participant_{participant_num}_data.pkl')
        with open(participant_file_path, 'wb') as file:
            pickle.dump(participant_data, file)

        print(f"Saved participant {participant_num} data in the file: {participant_file_path}")

"""Save chunk in the file // drive"""

participant_total = 15
n_chunks = 700
participant_num = 1

save_chunks_in_file(participant_total, n_chunks, participant_num)